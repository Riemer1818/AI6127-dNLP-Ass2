\documentclass{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{thmtools}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}
\usepackage{tcolorbox}
\usepackage{textcomp}
\usepackage{listings} 


\colorlet{LightGray}{White!90!Periwinkle}
\colorlet{LightOrange}{Orange!15}
\colorlet{LightGreen}{Green!15}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}

\declaretheoremstyle[name=Theorem,]{thmsty}
\declaretheorem[style=thmsty,numberwithin=section]{theorem}
\tcolorboxenvironment{theorem}{colback=LightGray}

\declaretheoremstyle[name=Proposition,]{prosty}
\declaretheorem[style=prosty,numberlike=theorem]{proposition}
\tcolorboxenvironment{proposition}{colback=LightOrange}

\declaretheoremstyle[name=Principle,]{prcpsty}
\declaretheorem[style=prcpsty,numberlike=theorem]{principle}
\tcolorboxenvironment{principle}{colback=LightGreen}

\setstretch{1.2}
\geometry{
    textheight=9in,
    textwidth=5.5in,
    top=1in,
    headheight=12pt,
    headsep=25pt,
    footskip=30pt
}

\begin{document}

% ====== TITLE PAGE ======
\title{ 
    \normalsize \textsc{} \\
    [2.0cm]
    \HRule{1.5pt} \\
    \LARGE \textbf{\uppercase{Seq2Seq Model for Machine Translation}} \\
    \HRule{2.0pt} \\ [0.6cm]
    \LARGE{AI6127: Deep Learning for Natural Language Processing} \\
    \vspace*{5\baselineskip}
}
\date{Submission Date: 23 April 2024}
\author{\textbf{Riemer van der Vliet} \\ 
    G2304212K}

\maketitle

\newpage

% ====== ABSTRACT ======
\section*{Abstract}
Briefly summarize the scope, methods, results, and conclusions of your report.

\section{Introduction}
This report focuses on the implementation and comparative analysis of various Seq2Seq models for the task of machine translation. We begin with a baseline model utilizing GRU for both the encoder and decoder components, renowned for their efficiency and capacity to mitigate vanishing gradient problems common in traditional RNNs. Subsequently, we explore the substitution of GRU with LSTM in the encoder and decoder, aiming to leverage LSTM's more complex gating mechanisms for better information flow. Additionally, the implementation includes a variant with a bidirectional LSTM (Bi-LSTM) in the encoder, designed to enhance the model's context understanding by processing the input sequence in both forward and reverse directions.

\section{Methodology}
In this report the following Seq2Seq models are implemented and compared for machine translation:
\begin{itemize}
    \item Original GRU-based model.
    \item LSTM replacement for GRU.
    \item Bi-LSTM in the Encoder.
    \item Addition of the attention mechanism.
    \item Transformer Encoder integration.
\end{itemize}

\section{Experiments}
\subsection{Experimental Setup}
\subsubsection{Dataset}
This dataset serves as a practical foundation for implementing and testing seq2seq models, specifically tailored towards converting sentences from English to French. The primary source of this data is Tatoeba.org, a community-driven project aimed at providing accessible translations for various sentences across numerous languages. The Tatoeba project's extensive database allows for a broad range of applications, from language learning tools to advanced research in machine translation.

For this project, the English-French translation pairs are available through an ancillary resource, http://www.manythings.org/anki/. This facilitates the direct application of these pairs in training seq2seq models by providing a simple, structured format. Each line in the dataset file contains an English sentence followed by its French translation, separated by a tab character, making it straightforward to parse and use in training algorithms.

    ``I am cold.    J'ai froid.``

The datasets are preprocessed to remove special characters, convert text to lowercase, and tokenize the sentences. The tokenization process involves splitting the sentences into individual words, which are then used to create a vocabulary for the model. The vocabulary is a set of unique words present in the dataset, which serves as the basis for encoding and decoding text sequences during training and inference.

\subsubsection{Rouge Scores}
ROUGE scores, an acronym for Recall-Oriented Understudy for Gisting Evaluation, are a set of metrics used to evaluate the quality of text generated by a model. The ROUGE scores are based on the comparison of n-grams between the generated text and the reference text. A higher ROUGE score indicates a better match between the generated text and the reference text.

The difference between rouge1 and rouge2 is the number of words in the n-gram. Rouge1 compares unigrams, while Rouge2 compares bigrams. The fmeasure, precision, and recall are the three metrics used to evaluate the quality of the generated text.

\subsection{Results}
Here the results are discussed for the methods mentioned in Methodology.
\textbf{Baseline GRU Model}
This Gated Recurrent Unit (GRU) model serves as the baseline for comparison with other models. GRUs are known for their efficiency and ability to mitigate vanishing gradient problems common in traditional RNNs.

% === Evaluation score - Rouge score ===
% Rouge1 fmeasure:	 0.62394214
% Rouge1 precision:	 0.5904058
% Rouge1 recall:  	 0.67221373
% Rouge2 fmeasure:	 0.43355218
% Rouge2 precision:	 0.40353066
% Rouge2 recall:  	 0.47851956


\textbf{LSTM Model}
This is particularly useful in tasks where the past context is important for predicting the future, such as time series forecasting or unidirectional language translation

% on test
% === Evaluation score - Rouge score ===
% Rouge1 fmeasure:	 0.60158986
% Rouge1 precision:	 0.5764233
% Rouge1 recall:  	 0.637757
% Rouge2 fmeasure:	 0.40399328
% Rouge2 precision:	 0.38184258
% Rouge2 recall:  	 0.43781084
% =====================================

% img/LSTM_learning.png



\textbf{Bi-LSTM Model}
This approach allows the network to have both forward-looking (future) and backward-looking (past) context at every point in the sequence

% === Evaluation score - Rouge score ===
% Rouge1 fmeasure:	 0.6203933
% Rouge1 precision:	 0.5911769
% Rouge1 recall:  	 0.6619157
% Rouge2 fmeasure:	 0.42324743
% Rouge2 precision:	 0.3964009
% Rouge2 recall:  	 0.4630641

% img/LSTMBi_learning.png

\textbf{Model with Attention}
Attention allows the model to focus on different parts of the input sequence for each step of the output sequence, providing a way to dynamically weigh the input's relevance. This is especially useful in translation where the importance of input words can vary depending on the context of the output being generated. 

% === Evaluation score - Rouge score ===
% Rouge1 fmeasure:	 0.68806785
% Rouge1 precision:	 0.65075564
% Rouge1 recall:  	 0.7393444
% Rouge2 fmeasure:	 0.51409614
% Rouge2 precision:	 0.47719076
% Rouge2 recall:  	 0.5673634

% img/Attention_learning.png

\textbf{Transformer Encoder Model}

GRUs process data sequentially, which inherently limits the speed of computation, especially for long sequences. 

\begin{table}[ht]
    \centering
    \begin{tabular}{lcccccc}
    \hline
    \textbf{Model} & \multicolumn{3}{c}{\textbf{Rouge1}} & \multicolumn{3}{c}{\textbf{Rouge2}} \\
     & \textbf{Fmeasure} & \textbf{Precision} & \textbf{Recall} & \textbf{Fmeasure} & \textbf{Precision} & \textbf{Recall} \\
    \hline
    Baseline GRU & 0.6239 & 0.5904 & 0.6722 & 0.4336 & 0.4035 & 0.4785 \\
    LSTM & 0.6016 & 0.5764 & 0.6378 & 0.4040 & 0.3818 & 0.4378 \\
    Bi-LSTM & 0.6204 & 0.5912 & 0.6619 & 0.4232 & 0.3964 & 0.4631 \\
    Model with Attention & 0.6881 & 0.6508 & 0.7393 & 0.5141 & 0.4772 & 0.5674 \\
    \hline
    \end{tabular}
    \caption{Evaluation scores - Rouge metrics for different models}
    \label{table:rouge_scores}
\end{table}



\begin{figure}[htbp]
    \centering
    \begin{minipage}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/GRU_learning.png}
        \caption{GRU Model}
        \label{fig:gru}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/LSTM_learning.png}
        \caption{LSTM Model}
        \label{fig:lstm}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/LSTMBi_learning.png}
        \caption{Bi-LSTM Model}
        \label{fig:bilstm}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/Attention_learning.png}
        \caption{Attention Model}
        \label{fig:attention}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/Attention_learning.png}
        \caption{Transformer Model}
        \label{fig:transformer}
    \end{minipage}
    \caption{Comparison of Seq2Seq Models}
    \label{fig:seq2seq_models}
\end{figure}

\section{Discussion}

As you can see in table \ref{table:rouge_scores}, the model with attention outperforms the other models in terms of Rouge scores. This is expected as the attention mechanism allows the model to focus on different parts of the input sequence for each step of the output sequence, providing a way to dynamically weigh the input's relevance. This is especially useful in translation where the importance of input words can vary depending on the context of the output being generated.

It's worth noting that the attention model scores particularly high on the Rouge2 set, which implies that attention has some additional benefits when it comes to bi gram matching. That none of the other models possess.

When it comes to learning there should be considered hyperpareter optimization, but as can be seen in \ref{fig:gru}, \ref{fig:lstm}, \ref{fig:bilstm}, \ref{fig:attention}, and \ref{fig:transformer} the models are learning at a similar pace and all converging. 

\section{Conclusion}

\section{References}
\bibliographystyle{IEEEtran}
% Include your references here

\section{Appendix}
% Any additional information

\end{document}